"""
SPARSA-LM Dataset Catalog
Comprehensive listing of datasets for pretraining, evaluation, SFT, and RLHF
Based on SmolLM-360M and other high-quality open datasets
"""

from dataclasses import dataclass, field
from typing import List, Dict, Optional
from enum import Enum


class DatasetPurpose(Enum):
    PRETRAIN = "pretrain"
    EVAL = "eval"
    SFT = "sft"
    RLHF = "rlhf"
    TOKENIZER = "tokenizer"


class License(Enum):
    APACHE_2 = "Apache-2.0"
    MIT = "MIT"
    CC_BY = "CC-BY-4.0"
    CC_BY_SA = "CC-BY-SA-4.0"
    CC_BY_NC = "CC-BY-NC-4.0"
    CC0 = "CC0-1.0"
    ODC_BY = "ODC-BY-1.0"
    OPENAI = "OpenAI Terms"
    LLAMA = "Llama 2 Community License"
    CUSTOM = "Custom"
    UNKNOWN = "Unknown"


@dataclass
class DatasetInfo:
    """Information about a dataset."""
    name: str
    hf_path: str
    source_url: str
    license: License
    size: str  # Human readable size
    num_tokens: Optional[str]  # Approximate token count
    purpose: List[DatasetPurpose]
    description: str
    subsets: Optional[List[str]] = None
    citation: Optional[str] = None


# =============================================================================
# PRE-TRAINING DATASETS
# =============================================================================

PRETRAIN_DATASETS: List[DatasetInfo] = [
    # -------------------------------------------------------------------------
    # SmolLM Training Datasets (Cosmopedia, FineWeb-Edu, etc.)
    # -------------------------------------------------------------------------
    DatasetInfo(
        name="Cosmopedia v2",
        hf_path="HuggingFaceTB/cosmopedia-v2",
        source_url="https://huggingface.co/datasets/HuggingFaceTB/cosmopedia-v2",
        license=License.APACHE_2,
        size="~28B tokens",
        num_tokens="28B",
        purpose=[DatasetPurpose.PRETRAIN],
        description="Synthetic textbooks, stories, and articles generated by Mixtral-8x7B. "
                    "High-quality educational content covering diverse topics.",
        subsets=["auto_math_text", "khanacademy", "openstax", "stanford", "stories", "wikihow"],
        citation="HuggingFaceTB, 2024"
    ),
    DatasetInfo(
        name="FineWeb-Edu",
        hf_path="HuggingFaceFW/fineweb-edu",
        source_url="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu",
        license=License.ODC_BY,
        size="~1.3T tokens",
        num_tokens="1.3T",
        purpose=[DatasetPurpose.PRETRAIN],
        description="Educational subset of FineWeb filtered using a classifier trained on "
                    "Llama-3-70B annotations. High-quality web content for education.",
        subsets=["sample-10BT", "sample-100BT", "sample-350BT"],
        citation="Penedo et al., 2024"
    ),
    DatasetInfo(
        name="FineWeb",
        hf_path="HuggingFaceFW/fineweb",
        source_url="https://huggingface.co/datasets/HuggingFaceFW/fineweb",
        license=License.ODC_BY,
        size="~15T tokens",
        num_tokens="15T",
        purpose=[DatasetPurpose.PRETRAIN],
        description="Large-scale web dataset with extensive deduplication and filtering. "
                    "Based on CommonCrawl with quality improvements.",
        citation="Penedo et al., 2024"
    ),
    DatasetInfo(
        name="The Stack v2",
        hf_path="bigcode/the-stack-v2",
        source_url="https://huggingface.co/datasets/bigcode/the-stack-v2",
        license=License.CUSTOM,
        size="~900B tokens",
        num_tokens="900B",
        purpose=[DatasetPurpose.PRETRAIN],
        description="Largest open code dataset with 619 programming languages. "
                    "Deduplicated and permissively licensed.",
        subsets=["python", "javascript", "java", "c", "cpp", "go", "rust", "typescript"],
        citation="Lozhkov et al., 2024"
    ),
    DatasetInfo(
        name="StarCoder Data",
        hf_path="bigcode/starcoderdata",
        source_url="https://huggingface.co/datasets/bigcode/starcoderdata",
        license=License.APACHE_2,
        size="~250B tokens",
        num_tokens="250B",
        purpose=[DatasetPurpose.PRETRAIN],
        description="Curated code dataset used for training StarCoder models. "
                    "86 programming languages with GitHub issues and commits.",
        citation="Li et al., 2023"
    ),
    DatasetInfo(
        name="Python-Edu",
        hf_path="HuggingFaceTB/smollm-corpus",
        source_url="https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus",
        license=License.APACHE_2,
        size="~4B tokens",
        num_tokens="4B",
        purpose=[DatasetPurpose.PRETRAIN],
        description="Educational Python code filtered from The Stack using a classifier. "
                    "Part of SmolLM training corpus.",
        subsets=["python-edu"],
        citation="HuggingFaceTB, 2024"
    ),
    # -------------------------------------------------------------------------
    # General Web Datasets
    # -------------------------------------------------------------------------
    DatasetInfo(
        name="RedPajama v2",
        hf_path="togethercomputer/RedPajama-Data-V2",
        source_url="https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2",
        license=License.APACHE_2,
        size="~30T tokens",
        num_tokens="30T",
        purpose=[DatasetPurpose.PRETRAIN],
        description="Web-scale dataset from 84 CommonCrawl snapshots with quality signals "
                    "and deduplication.",
        citation="Together AI, 2023"
    ),
    DatasetInfo(
        name="C4",
        hf_path="allenai/c4",
        source_url="https://huggingface.co/datasets/allenai/c4",
        license=License.ODC_BY,
        size="~156B tokens",
        num_tokens="156B",
        purpose=[DatasetPurpose.PRETRAIN],
        description="Colossal Clean Crawled Corpus. Cleaned version of Common Crawl "
                    "used for training T5.",
        subsets=["en", "en.noblocklist", "en.noclean"],
        citation="Raffel et al., 2020"
    ),
    DatasetInfo(
        name="The Pile",
        hf_path="EleutherAI/pile",
        source_url="https://huggingface.co/datasets/EleutherAI/pile",
        license=License.MIT,
        size="~825GB / ~300B tokens",
        num_tokens="300B",
        purpose=[DatasetPurpose.PRETRAIN],
        description="Diverse 825GB dataset from 22 sources including academic, code, "
                    "books, and web data.",
        subsets=["pile-cc", "pubmed", "arxiv", "github", "stackexchange", "wikipedia"],
        citation="Gao et al., 2020"
    ),
    DatasetInfo(
        name="Dolma",
        hf_path="allenai/dolma",
        source_url="https://huggingface.co/datasets/allenai/dolma",
        license=License.ODC_BY,
        size="~3T tokens",
        num_tokens="3T",
        purpose=[DatasetPurpose.PRETRAIN],
        description="Open dataset for language model pretraining from AI2. "
                    "Includes web, code, books, and academic papers.",
        citation="Soldaini et al., 2024"
    ),
    DatasetInfo(
        name="SlimPajama",
        hf_path="cerebras/SlimPajama-627B",
        source_url="https://huggingface.co/datasets/cerebras/SlimPajama-627B",
        license=License.APACHE_2,
        size="~627B tokens",
        num_tokens="627B",
        purpose=[DatasetPurpose.PRETRAIN],
        description="Cleaned and deduplicated version of RedPajama. "
                    "49.6% smaller with improved quality.",
        citation="Cerebras, 2023"
    ),
    # -------------------------------------------------------------------------
    # Wikipedia and Books
    # -------------------------------------------------------------------------
    DatasetInfo(
        name="Wikipedia",
        hf_path="wikipedia",
        source_url="https://huggingface.co/datasets/wikipedia",
        license=License.CC_BY_SA,
        size="~21GB (English)",
        num_tokens="~4B",
        purpose=[DatasetPurpose.PRETRAIN, DatasetPurpose.TOKENIZER],
        description="Wikipedia articles in multiple languages. High-quality encyclopedic content.",
        subsets=["20220301.en", "20220301.de", "20220301.fr"],
        citation="Wikimedia Foundation"
    ),
    DatasetInfo(
        name="OpenWebText2",
        hf_path="Skylion007/openwebtext",
        source_url="https://huggingface.co/datasets/Skylion007/openwebtext",
        license=License.CC0,
        size="~38GB",
        num_tokens="~8B",
        purpose=[DatasetPurpose.PRETRAIN],
        description="Open-source recreation of WebText dataset. "
                    "Reddit submissions with 3+ karma.",
        citation="Gokaslan et al., 2019"
    ),
    DatasetInfo(
        name="BookCorpus",
        hf_path="bookcorpusopen",
        source_url="https://huggingface.co/datasets/bookcorpusopen",
        license=License.MIT,
        size="~5GB",
        num_tokens="~1B",
        purpose=[DatasetPurpose.PRETRAIN],
        description="Collection of free books from smashwords.com. "
                    "Long-form narrative content.",
        citation="Zhu et al., 2015"
    ),
    DatasetInfo(
        name="Gutenberg",
        hf_path="pg19",
        source_url="https://huggingface.co/datasets/pg19",
        license=License.APACHE_2,
        size="~11GB",
        num_tokens="~2B",
        purpose=[DatasetPurpose.PRETRAIN],
        description="Project Gutenberg books. Public domain literature and non-fiction.",
        citation="Rae et al., 2020"
    ),
    # -------------------------------------------------------------------------
    # Math and Science
    # -------------------------------------------------------------------------
    DatasetInfo(
        name="OpenWebMath",
        hf_path="open-web-math/open-web-math",
        source_url="https://huggingface.co/datasets/open-web-math/open-web-math",
        license=License.ODC_BY,
        size="~14.7B tokens",
        num_tokens="14.7B",
        purpose=[DatasetPurpose.PRETRAIN],
        description="Mathematical web pages from Common Crawl. "
                    "LaTeX, MathML, and ASCII math expressions.",
        citation="Paster et al., 2023"
    ),
    DatasetInfo(
        name="Proof-Pile-2",
        hf_path="EleutherAI/proof-pile-2",
        source_url="https://huggingface.co/datasets/EleutherAI/proof-pile-2",
        license=License.MIT,
        size="~55B tokens",
        num_tokens="55B",
        purpose=[DatasetPurpose.PRETRAIN],
        description="Mathematical and scientific papers, books, and code. "
                    "Includes ArXiv, Wikipedia math, and formal proofs.",
        citation="Azerbayev et al., 2023"
    ),
    DatasetInfo(
        name="peS2o",
        hf_path="allenai/peS2o",
        source_url="https://huggingface.co/datasets/allenai/peS2o",
        license=License.ODC_BY,
        size="~40M papers",
        num_tokens="~38B",
        purpose=[DatasetPurpose.PRETRAIN],
        description="Scientific papers from Semantic Scholar. "
                    "39.8M open access papers across disciplines.",
        citation="Soldaini et al., 2024"
    ),
    DatasetInfo(
        name="ArXiv",
        hf_path="arxiv_dataset",
        source_url="https://huggingface.co/datasets/arxiv_dataset",
        license=License.CC0,
        size="~60GB",
        num_tokens="~15B",
        purpose=[DatasetPurpose.PRETRAIN],
        description="Scientific papers from ArXiv. Physics, math, CS, and more.",
        citation="ArXiv.org"
    ),
]


# =============================================================================
# EVALUATION BENCHMARKS
# =============================================================================

EVAL_DATASETS: List[DatasetInfo] = [
    # -------------------------------------------------------------------------
    # Commonsense Reasoning
    # -------------------------------------------------------------------------
    DatasetInfo(
        name="ARC (AI2 Reasoning Challenge)",
        hf_path="allenai/ai2_arc",
        source_url="https://huggingface.co/datasets/allenai/ai2_arc",
        license=License.CC_BY_SA,
        size="7,787 questions",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="Science exam questions from grades 3-9. "
                    "ARC-Easy and ARC-Challenge splits.",
        subsets=["ARC-Easy", "ARC-Challenge"],
        citation="Clark et al., 2018"
    ),
    DatasetInfo(
        name="HellaSwag",
        hf_path="Rowan/hellaswag",
        source_url="https://huggingface.co/datasets/Rowan/hellaswag",
        license=License.MIT,
        size="70,000 questions",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="Commonsense NLI: sentence completion requiring physical and social reasoning.",
        citation="Zellers et al., 2019"
    ),
    DatasetInfo(
        name="WinoGrande",
        hf_path="winogrande",
        source_url="https://huggingface.co/datasets/winogrande",
        license=License.CC_BY,
        size="44,000 questions",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="Large-scale Winograd schema challenge for coreference resolution.",
        subsets=["winogrande_xs", "winogrande_s", "winogrande_m", "winogrande_l", "winogrande_xl"],
        citation="Sakaguchi et al., 2020"
    ),
    DatasetInfo(
        name="PIQA",
        hf_path="piqa",
        source_url="https://huggingface.co/datasets/piqa",
        license=License.APACHE_2,
        size="21,000 questions",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="Physical Intuition QA: physical commonsense reasoning.",
        citation="Bisk et al., 2020"
    ),
    DatasetInfo(
        name="SIQA",
        hf_path="social_i_qa",
        source_url="https://huggingface.co/datasets/social_i_qa",
        license=License.CC_BY,
        size="38,000 questions",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="Social Intelligence QA: social commonsense reasoning.",
        citation="Sap et al., 2019"
    ),
    DatasetInfo(
        name="CommonsenseQA",
        hf_path="commonsense_qa",
        source_url="https://huggingface.co/datasets/commonsense_qa",
        license=License.MIT,
        size="12,247 questions",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="Commonsense QA requiring world knowledge from ConceptNet.",
        citation="Talmor et al., 2019"
    ),
    DatasetInfo(
        name="OpenBookQA",
        hf_path="openbookqa",
        source_url="https://huggingface.co/datasets/openbookqa",
        license=License.APACHE_2,
        size="6,000 questions",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="Science QA requiring open book reasoning and external knowledge.",
        citation="Mihaylov et al., 2018"
    ),
    DatasetInfo(
        name="BoolQ",
        hf_path="boolq",
        source_url="https://huggingface.co/datasets/boolq",
        license=License.CC_BY_SA,
        size="15,942 questions",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="Boolean QA with naturally occurring yes/no questions from Google search.",
        citation="Clark et al., 2019"
    ),
    # -------------------------------------------------------------------------
    # Knowledge and World Understanding
    # -------------------------------------------------------------------------
    DatasetInfo(
        name="MMLU",
        hf_path="cais/mmlu",
        source_url="https://huggingface.co/datasets/cais/mmlu",
        license=License.MIT,
        size="15,908 questions (57 subjects)",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="Massive Multitask Language Understanding across 57 academic subjects.",
        subsets=["all", "stem", "humanities", "social_sciences", "other"],
        citation="Hendrycks et al., 2021"
    ),
    DatasetInfo(
        name="TriviaQA",
        hf_path="trivia_qa",
        source_url="https://huggingface.co/datasets/trivia_qa",
        license=License.APACHE_2,
        size="650,000 QA pairs",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="Large-scale reading comprehension from trivia questions.",
        subsets=["rc", "rc.nocontext", "unfiltered", "unfiltered.nocontext"],
        citation="Joshi et al., 2017"
    ),
    DatasetInfo(
        name="NaturalQuestions",
        hf_path="natural_questions",
        source_url="https://huggingface.co/datasets/natural_questions",
        license=License.CC_BY_SA,
        size="323,045 questions",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="Real Google search queries with Wikipedia answers.",
        citation="Kwiatkowski et al., 2019"
    ),
    DatasetInfo(
        name="TruthfulQA",
        hf_path="truthful_qa",
        source_url="https://huggingface.co/datasets/truthful_qa",
        license=License.APACHE_2,
        size="817 questions",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="Truthfulness evaluation: questions designed to elicit false answers.",
        subsets=["generation", "multiple_choice"],
        citation="Lin et al., 2022"
    ),
    # -------------------------------------------------------------------------
    # Mathematics
    # -------------------------------------------------------------------------
    DatasetInfo(
        name="GSM8K",
        hf_path="gsm8k",
        source_url="https://huggingface.co/datasets/gsm8k",
        license=License.MIT,
        size="8,500 problems",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="Grade school math word problems requiring multi-step reasoning.",
        subsets=["main", "socratic"],
        citation="Cobbe et al., 2021"
    ),
    DatasetInfo(
        name="MATH",
        hf_path="hendrycks/competition_math",
        source_url="https://huggingface.co/datasets/hendrycks/competition_math",
        license=License.MIT,
        size="12,500 problems",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="Competition mathematics from AMC, AIME, and Olympiads.",
        citation="Hendrycks et al., 2021"
    ),
    DatasetInfo(
        name="MathQA",
        hf_path="math_qa",
        source_url="https://huggingface.co/datasets/math_qa",
        license=License.APACHE_2,
        size="37,000 problems",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="Math word problems with operation annotations from AQuA-RAT.",
        citation="Amini et al., 2019"
    ),
    # -------------------------------------------------------------------------
    # Code
    # -------------------------------------------------------------------------
    DatasetInfo(
        name="HumanEval",
        hf_path="openai_humaneval",
        source_url="https://huggingface.co/datasets/openai_humaneval",
        license=License.MIT,
        size="164 problems",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="Python function completion with unit tests. OpenAI benchmark.",
        citation="Chen et al., 2021"
    ),
    DatasetInfo(
        name="MBPP",
        hf_path="mbpp",
        source_url="https://huggingface.co/datasets/mbpp",
        license=License.CC_BY,
        size="974 problems",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="Mostly Basic Python Problems: entry-level Python programming.",
        subsets=["full", "sanitized"],
        citation="Austin et al., 2021"
    ),
    DatasetInfo(
        name="MultiPL-E",
        hf_path="nuprl/MultiPL-E",
        source_url="https://huggingface.co/datasets/nuprl/MultiPL-E",
        license=License.MIT,
        size="164 problems x 18 languages",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="HumanEval translated to 18 programming languages.",
        citation="Cassano et al., 2023"
    ),
    # -------------------------------------------------------------------------
    # Language Understanding
    # -------------------------------------------------------------------------
    DatasetInfo(
        name="LAMBADA",
        hf_path="lambada",
        source_url="https://huggingface.co/datasets/lambada",
        license=License.CC_BY,
        size="10,022 passages",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="Language modeling with long-range dependencies. Word prediction.",
        citation="Paperno et al., 2016"
    ),
    DatasetInfo(
        name="SQuAD",
        hf_path="squad",
        source_url="https://huggingface.co/datasets/squad",
        license=License.CC_BY_SA,
        size="100,000+ questions",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="Stanford Question Answering Dataset: reading comprehension.",
        subsets=["plain_text"],
        citation="Rajpurkar et al., 2016"
    ),
    DatasetInfo(
        name="SuperGLUE",
        hf_path="super_glue",
        source_url="https://huggingface.co/datasets/super_glue",
        license=License.CUSTOM,
        size="~32,000 examples",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="Challenging NLU benchmark: BoolQ, CB, COPA, MultiRC, ReCoRD, RTE, WiC, WSC.",
        subsets=["boolq", "cb", "copa", "multirc", "record", "rte", "wic", "wsc"],
        citation="Wang et al., 2019"
    ),
    DatasetInfo(
        name="DROP",
        hf_path="drop",
        source_url="https://huggingface.co/datasets/drop",
        license=License.CC_BY_SA,
        size="96,000 questions",
        num_tokens=None,
        purpose=[DatasetPurpose.EVAL],
        description="Discrete reasoning over paragraphs: numerical and comparative reasoning.",
        citation="Dua et al., 2019"
    ),
]


# =============================================================================
# SUPERVISED FINE-TUNING DATASETS
# =============================================================================

SFT_DATASETS: List[DatasetInfo] = [
    DatasetInfo(
        name="OpenHermes 2.5",
        hf_path="teknium/OpenHermes-2.5",
        source_url="https://huggingface.co/datasets/teknium/OpenHermes-2.5",
        license=License.APACHE_2,
        size="1M+ conversations",
        num_tokens="~1B",
        purpose=[DatasetPurpose.SFT],
        description="Large-scale instruction dataset from multiple sources. "
                    "Includes code, math, reasoning, and general instructions.",
        citation="Teknium, 2023"
    ),
    DatasetInfo(
        name="UltraChat 200K",
        hf_path="HuggingFaceH4/ultrachat_200k",
        source_url="https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k",
        license=License.MIT,
        size="200K conversations",
        num_tokens="~500M",
        purpose=[DatasetPurpose.SFT],
        description="Filtered version of UltraChat with high-quality multi-turn conversations.",
        citation="Ding et al., 2023"
    ),
    DatasetInfo(
        name="SlimOrca",
        hf_path="Open-Orca/SlimOrca",
        source_url="https://huggingface.co/datasets/Open-Orca/SlimOrca",
        license=License.MIT,
        size="518K examples",
        num_tokens="~400M",
        purpose=[DatasetPurpose.SFT],
        description="Curated subset of OpenOrca with deduplication and quality filtering.",
        citation="Open-Orca, 2023"
    ),
    DatasetInfo(
        name="Dolly 15K",
        hf_path="databricks/databricks-dolly-15k",
        source_url="https://huggingface.co/datasets/databricks/databricks-dolly-15k",
        license=License.CC_BY_SA,
        size="15,000 examples",
        num_tokens="~10M",
        purpose=[DatasetPurpose.SFT],
        description="Human-generated instruction-following dataset from Databricks employees.",
        citation="Databricks, 2023"
    ),
    DatasetInfo(
        name="OpenAssistant Conversations",
        hf_path="OpenAssistant/oasst1",
        source_url="https://huggingface.co/datasets/OpenAssistant/oasst1",
        license=License.APACHE_2,
        size="161,000 messages",
        num_tokens="~100M",
        purpose=[DatasetPurpose.SFT],
        description="Human-generated assistant conversations with quality ratings. "
                    "Multi-turn dialogue trees.",
        citation="KÃ¶pf et al., 2023"
    ),
    DatasetInfo(
        name="Alpaca",
        hf_path="tatsu-lab/alpaca",
        source_url="https://huggingface.co/datasets/tatsu-lab/alpaca",
        license=License.CC_BY_NC,
        size="52K examples",
        num_tokens="~30M",
        purpose=[DatasetPurpose.SFT],
        description="Instruction-following data generated using Self-Instruct with GPT-3.5.",
        citation="Taori et al., 2023"
    ),
    DatasetInfo(
        name="WizardLM Evol-Instruct",
        hf_path="WizardLM/WizardLM_evol_instruct_V2_196k",
        source_url="https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k",
        license=License.APACHE_2,
        size="196K examples",
        num_tokens="~200M",
        purpose=[DatasetPurpose.SFT],
        description="Evol-Instruct method: progressively more complex instructions.",
        citation="Xu et al., 2023"
    ),
    DatasetInfo(
        name="ShareGPT",
        hf_path="anon8231489123/ShareGPT_Vicuna_unfiltered",
        source_url="https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered",
        license=License.CC_BY_NC,
        size="~70K conversations",
        num_tokens="~500M",
        purpose=[DatasetPurpose.SFT],
        description="User-shared ChatGPT conversations. Multi-turn dialogue.",
        citation="ShareGPT Community"
    ),
    DatasetInfo(
        name="Code Alpaca",
        hf_path="sahil2801/CodeAlpaca-20k",
        source_url="https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k",
        license=License.APACHE_2,
        size="20K examples",
        num_tokens="~15M",
        purpose=[DatasetPurpose.SFT],
        description="Code instruction-following dataset generated using Self-Instruct.",
        citation="Sahil, 2023"
    ),
    DatasetInfo(
        name="MetaMath",
        hf_path="meta-math/MetaMathQA",
        source_url="https://huggingface.co/datasets/meta-math/MetaMathQA",
        license=License.MIT,
        size="395K examples",
        num_tokens="~100M",
        purpose=[DatasetPurpose.SFT],
        description="Math instruction data with question rephrasing and answer augmentation.",
        citation="Yu et al., 2023"
    ),
    DatasetInfo(
        name="MathInstruct",
        hf_path="TIGER-Lab/MathInstruct",
        source_url="https://huggingface.co/datasets/TIGER-Lab/MathInstruct",
        license=License.MIT,
        size="260K examples",
        num_tokens="~80M",
        purpose=[DatasetPurpose.SFT],
        description="Math reasoning dataset with chain-of-thought and program-of-thought.",
        citation="Yue et al., 2023"
    ),
    DatasetInfo(
        name="Magicoder Evol-Instruct",
        hf_path="ise-uiuc/Magicoder-Evol-Instruct-110K",
        source_url="https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K",
        license=License.MIT,
        size="110K examples",
        num_tokens="~100M",
        purpose=[DatasetPurpose.SFT],
        description="Code instruction data using OSS-Instruct with open-source code snippets.",
        citation="Wei et al., 2023"
    ),
]


# =============================================================================
# RLHF/RLAIF DATASETS
# =============================================================================

RLHF_DATASETS: List[DatasetInfo] = [
    # -------------------------------------------------------------------------
    # Preference Data for RLHF
    # -------------------------------------------------------------------------
    DatasetInfo(
        name="Anthropic HH-RLHF",
        hf_path="Anthropic/hh-rlhf",
        source_url="https://huggingface.co/datasets/Anthropic/hh-rlhf",
        license=License.MIT,
        size="170K comparisons",
        num_tokens=None,
        purpose=[DatasetPurpose.RLHF],
        description="Human preference data for helpfulness and harmlessness. "
                    "Red-teaming data included.",
        subsets=["helpful-base", "helpful-rejection-sampled", "helpful-online",
                 "harmless-base", "harmless-rejection-sampled", "harmless-online"],
        citation="Bai et al., 2022"
    ),
    DatasetInfo(
        name="UltraFeedback",
        hf_path="openbmb/UltraFeedback",
        source_url="https://huggingface.co/datasets/openbmb/UltraFeedback",
        license=License.MIT,
        size="64K prompts, 256K responses",
        num_tokens=None,
        purpose=[DatasetPurpose.RLHF],
        description="AI feedback from GPT-4 on responses from 17 LLMs. "
                    "Multi-aspect ratings: instruction-following, truthfulness, honesty, helpfulness.",
        citation="Cui et al., 2023"
    ),
    DatasetInfo(
        name="UltraFeedback Binarized",
        hf_path="HuggingFaceH4/ultrafeedback_binarized",
        source_url="https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized",
        license=License.MIT,
        size="64K preference pairs",
        num_tokens=None,
        purpose=[DatasetPurpose.RLHF],
        description="Binarized version of UltraFeedback for DPO training. "
                    "Used for Zephyr and Notus models.",
        citation="Tunstall et al., 2023"
    ),
    DatasetInfo(
        name="Nectar",
        hf_path="berkeley-nest/Nectar",
        source_url="https://huggingface.co/datasets/berkeley-nest/Nectar",
        license=License.CC_BY_NC,
        size="183K prompts, 7 responses each",
        num_tokens=None,
        purpose=[DatasetPurpose.RLHF],
        description="GPT-4 ranked responses from 7 LLMs. Diverse prompts from multiple sources.",
        citation="Starling Team, 2023"
    ),
    DatasetInfo(
        name="OpenAI Summarize Comparisons",
        hf_path="openai/summarize_from_feedback",
        source_url="https://huggingface.co/datasets/openai/summarize_from_feedback",
        license=License.OPENAI,
        size="92K comparisons",
        num_tokens=None,
        purpose=[DatasetPurpose.RLHF],
        description="Human comparisons for TL;DR summarization. "
                    "Used in original RLHF paper.",
        citation="Stiennon et al., 2020"
    ),
    DatasetInfo(
        name="Stack Exchange Preferences",
        hf_path="lvwerra/stack-exchange-paired",
        source_url="https://huggingface.co/datasets/lvwerra/stack-exchange-paired",
        license=License.CC_BY_SA,
        size="10M pairs",
        num_tokens=None,
        purpose=[DatasetPurpose.RLHF],
        description="Stack Exchange answer pairs ranked by score. "
                    "Real human preferences from voting.",
        citation="von Werra et al., 2023"
    ),
    DatasetInfo(
        name="Argilla DPO Mix",
        hf_path="argilla/dpo-mix-7k",
        source_url="https://huggingface.co/datasets/argilla/dpo-mix-7k",
        license=License.APACHE_2,
        size="7K examples",
        num_tokens=None,
        purpose=[DatasetPurpose.RLHF],
        description="Mixed DPO dataset from multiple sources with quality filtering.",
        citation="Argilla, 2024"
    ),
    # -------------------------------------------------------------------------
    # RLAIF-Specific Datasets
    # -------------------------------------------------------------------------
    DatasetInfo(
        name="Capybara",
        hf_path="LDJnr/Capybara",
        source_url="https://huggingface.co/datasets/LDJnr/Capybara",
        license=License.APACHE_2,
        size="16K conversations",
        num_tokens=None,
        purpose=[DatasetPurpose.RLHF, DatasetPurpose.SFT],
        description="Multi-turn conversation dataset for RLAIF. "
                    "High-quality synthetic dialogues.",
        citation="LDJnr, 2023"
    ),
    DatasetInfo(
        name="ORCA DPO Pairs",
        hf_path="Intel/orca_dpo_pairs",
        source_url="https://huggingface.co/datasets/Intel/orca_dpo_pairs",
        license=License.MIT,
        size="12K pairs",
        num_tokens=None,
        purpose=[DatasetPurpose.RLHF],
        description="DPO pairs from ORCA dataset with GPT-4 as judge.",
        citation="Intel, 2023"
    ),
    DatasetInfo(
        name="Distilabel Math Preference DPO",
        hf_path="argilla/distilabel-math-preference-dpo",
        source_url="https://huggingface.co/datasets/argilla/distilabel-math-preference-dpo",
        license=License.APACHE_2,
        size="2.4K pairs",
        num_tokens=None,
        purpose=[DatasetPurpose.RLHF],
        description="Math-focused DPO pairs with AI-generated preferences.",
        citation="Argilla, 2024"
    ),
]


# =============================================================================
# DAPO-SPECIFIC DATASETS
# =============================================================================

DAPO_DATASETS: List[DatasetInfo] = [
    DatasetInfo(
        name="UltraFeedback for DAPO",
        hf_path="HuggingFaceH4/ultrafeedback_binarized",
        source_url="https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized",
        license=License.MIT,
        size="64K pairs",
        num_tokens=None,
        purpose=[DatasetPurpose.RLHF],
        description="Primary dataset for DAPO training with decoupled clipping. "
                    "Best for entropy-preserving optimization.",
        citation="DAPO Paper, 2024"
    ),
    DatasetInfo(
        name="Anthropic HH for DAPO",
        hf_path="Anthropic/hh-rlhf",
        source_url="https://huggingface.co/datasets/Anthropic/hh-rlhf",
        license=License.MIT,
        size="170K pairs",
        num_tokens=None,
        purpose=[DatasetPurpose.RLHF],
        description="Helpfulness/harmlessness data suited for DAPO's dynamic sampling.",
        subsets=["helpful-base", "harmless-base"],
        citation="DAPO Paper, 2024"
    ),
]


# =============================================================================
# VAPO-SPECIFIC DATASETS
# =============================================================================

VAPO_DATASETS: List[DatasetInfo] = [
    DatasetInfo(
        name="Nectar for VAPO",
        hf_path="berkeley-nest/Nectar",
        source_url="https://huggingface.co/datasets/berkeley-nest/Nectar",
        license=License.CC_BY_NC,
        size="183K prompts",
        num_tokens=None,
        purpose=[DatasetPurpose.RLHF],
        description="Multi-response dataset ideal for VAPO's dense reward training. "
                    "Value model can learn from diverse completions.",
        citation="VAPO Paper, 2024"
    ),
    DatasetInfo(
        name="Stack Exchange for VAPO",
        hf_path="lvwerra/stack-exchange-paired",
        source_url="https://huggingface.co/datasets/lvwerra/stack-exchange-paired",
        license=License.CC_BY_SA,
        size="10M pairs",
        num_tokens=None,
        purpose=[DatasetPurpose.RLHF],
        description="Large-scale preference data for VAPO's per-token reward signals. "
                    "Natural human preferences from voting.",
        citation="VAPO Paper, 2024"
    ),
]


# =============================================================================
# TOKENIZER TRAINING DATASETS
# =============================================================================

TOKENIZER_DATASETS: List[DatasetInfo] = [
    DatasetInfo(
        name="Wikipedia (EN)",
        hf_path="wikipedia",
        source_url="https://huggingface.co/datasets/wikipedia",
        license=License.CC_BY_SA,
        size="21GB",
        num_tokens="4B",
        purpose=[DatasetPurpose.TOKENIZER],
        description="High-quality encyclopedic text for vocabulary coverage.",
        subsets=["20220301.en"],
        citation="Wikimedia Foundation"
    ),
    DatasetInfo(
        name="BookCorpus",
        hf_path="bookcorpusopen",
        source_url="https://huggingface.co/datasets/bookcorpusopen",
        license=License.MIT,
        size="5GB",
        num_tokens="1B",
        purpose=[DatasetPurpose.TOKENIZER],
        description="Narrative text for diverse vocabulary.",
        citation="Zhu et al., 2015"
    ),
    DatasetInfo(
        name="OpenWebText",
        hf_path="Skylion007/openwebtext",
        source_url="https://huggingface.co/datasets/Skylion007/openwebtext",
        license=License.CC0,
        size="38GB",
        num_tokens="8B",
        purpose=[DatasetPurpose.TOKENIZER],
        description="Web text for broad vocabulary coverage.",
        citation="Gokaslan et al., 2019"
    ),
    DatasetInfo(
        name="The Stack (Python)",
        hf_path="bigcode/the-stack-v2",
        source_url="https://huggingface.co/datasets/bigcode/the-stack-v2",
        license=License.CUSTOM,
        size="~100GB (Python)",
        num_tokens="~50B",
        purpose=[DatasetPurpose.TOKENIZER],
        description="Code for programming vocabulary coverage.",
        subsets=["python"],
        citation="Lozhkov et al., 2024"
    ),
    DatasetInfo(
        name="ArXiv",
        hf_path="arxiv_dataset",
        source_url="https://huggingface.co/datasets/arxiv_dataset",
        license=License.CC0,
        size="60GB",
        num_tokens="15B",
        purpose=[DatasetPurpose.TOKENIZER],
        description="Scientific text for technical vocabulary.",
        citation="ArXiv.org"
    ),
]


# =============================================================================
# DATASET REGISTRY
# =============================================================================

class DatasetRegistry:
    """Registry for accessing all datasets."""

    @staticmethod
    def get_pretrain_datasets() -> List[DatasetInfo]:
        return PRETRAIN_DATASETS

    @staticmethod
    def get_eval_datasets() -> List[DatasetInfo]:
        return EVAL_DATASETS

    @staticmethod
    def get_sft_datasets() -> List[DatasetInfo]:
        return SFT_DATASETS

    @staticmethod
    def get_rlhf_datasets() -> List[DatasetInfo]:
        return RLHF_DATASETS

    @staticmethod
    def get_dapo_datasets() -> List[DatasetInfo]:
        return DAPO_DATASETS

    @staticmethod
    def get_vapo_datasets() -> List[DatasetInfo]:
        return VAPO_DATASETS

    @staticmethod
    def get_tokenizer_datasets() -> List[DatasetInfo]:
        return TOKENIZER_DATASETS

    @staticmethod
    def get_all_datasets() -> Dict[str, List[DatasetInfo]]:
        return {
            "pretrain": PRETRAIN_DATASETS,
            "eval": EVAL_DATASETS,
            "sft": SFT_DATASETS,
            "rlhf": RLHF_DATASETS,
            "dapo": DAPO_DATASETS,
            "vapo": VAPO_DATASETS,
            "tokenizer": TOKENIZER_DATASETS,
        }

    @staticmethod
    def get_smollm_datasets() -> List[DatasetInfo]:
        """Get datasets used by SmolLM-360M."""
        smollm_names = [
            "Cosmopedia v2", "FineWeb-Edu", "The Stack v2", "Python-Edu"
        ]
        return [d for d in PRETRAIN_DATASETS if d.name in smollm_names]

    @staticmethod
    def print_catalog():
        """Print formatted dataset catalog."""
        print("=" * 80)
        print("SPARSA-LM DATASET CATALOG")
        print("=" * 80)

        for category, datasets in DatasetRegistry.get_all_datasets().items():
            print(f"\n{'=' * 40}")
            print(f"{category.upper()} DATASETS ({len(datasets)})")
            print("=" * 40)

            for ds in datasets:
                print(f"\n{ds.name}")
                print(f"  Path: {ds.hf_path}")
                print(f"  License: {ds.license.value}")
                print(f"  Size: {ds.size}")
                if ds.num_tokens:
                    print(f"  Tokens: {ds.num_tokens}")
                print(f"  Purpose: {', '.join(p.value for p in ds.purpose)}")
                print(f"  Description: {ds.description[:100]}...")


if __name__ == "__main__":
    DatasetRegistry.print_catalog()
