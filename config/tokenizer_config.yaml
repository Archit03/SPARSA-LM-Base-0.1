# SPARSA-LM BPE Tokenizer Configuration

tokenizer:
  vocab_size: 32768
  min_frequency: 2
  special_tokens:
    - "[PAD]"
    - "[UNK]"
    - "[CLS]"
    - "[SEP]"
    - "[MASK]"
    - "[BOS]"
    - "[EOS]"


datasets:
  # Literature & narrative
  - name: "tiny_shakespeare"
    split: "train"
    text_column: "text"
    max_samples: 5000

  - name: "bookcorpusopen"
    split: "train"
    text_column: "text"
    max_samples: 50000

  # Wikipedia reference
  - name: "wikitext"
    subset: "wikitext-103-raw-v1"
    split: "train"
    text_column: "text"
    max_samples: 80000

  # Contemporary news
  - name: "cc_news"
    split: "train"
    text_column: "text"
    max_samples: 60000

  # Web text
  - name: "openwebtext"
    split: "train"
    text_column: "text"
    max_samples: 100000

  - name: "c4"
    subset: "en"
    split: "train"
    text_column: "text"
    max_samples: 80000

  # Scientific papers
  - name: "scientific_papers"
    subset: "pubmed"
    split: "train"
    text_column: "abstract"
    max_samples: 30000

  - name: "scientific_papers"
    subset: "arxiv"
    split: "train"
    text_column: "abstract"
    max_samples: 30000

  # Instruction & conversation
  - name: "databricks/databricks-dolly-15k"
    split: "train"
    text_column: "response"
    max_samples: 15000

  - name: "OpenAssistant/oasst1"
    split: "train"
    text_column: "text"
    max_samples: 25000

# Paths
output_dir: "data/processed/tokenizer"
cache_dir: ".cache/datasets"