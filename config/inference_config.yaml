# config/inference_config.yaml
# SPARSA-LM 360M Inference Configuration

# Model settings
model:
  path: "checkpoints/sparsa-360m"
  tokenizer_path: "tokenizer"
  dtype: "bfloat16"  # bfloat16, float16, float32
  hidden_dim: 1024
  num_layers: 28
  num_heads: 16
  num_kv_heads: 8
  ff_dim: 4096
  vocab_size: 32000
  max_seq_len: 2048

# Hardware settings
hardware:
  device: "cuda"
  tensor_parallel_size: 4  # For 4x L4 GPUs
  gpu_memory_utilization: 0.90
  max_num_gpus: 4

# vLLM settings
vllm:
  enabled: true
  max_model_len: 4096
  max_num_seqs: 256
  enable_chunked_prefill: true
  enable_prefix_caching: true
  disable_log_stats: false

# Generation defaults
generation:
  max_tokens: 2048
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.1
  presence_penalty: 0.0
  frequency_penalty: 0.0
  do_sample: true
  num_beams: 1
  early_stopping: true
  stop_sequences:
    - "<|end|>"
    - "<|user|>"

# Batching
batching:
  max_batch_size: 32
  continuous_batching: true
  dynamic_batching: true
  max_batch_wait_time_ms: 50

# Speculative decoding (optional)
speculative:
  enabled: false
  draft_model: null
  num_speculative_tokens: 5

# Server settings
server:
  host: "0.0.0.0"
  port: 8000
  workers: 1
  timeout: 300
  cors_origins:
    - "*"

# API settings
api:
  openai_compatible: true
  enable_streaming: true
  enable_chat: true
  enable_completions: true

# RAG settings
rag:
  enabled: true
  vector_store: "faiss"  # faiss or chromadb
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
  top_k: 5
  similarity_threshold: 0.5
  chunk_size: 512
  chunk_overlap: 50
  persist_directory: "data/vector_store"

# Logging
logging:
  level: "INFO"
  log_requests: true
  log_responses: false
  log_file: "logs/inference.log"
  use_wandb: false
  wandb_project: "SPARSA-LM-360M"

# Memory monitoring
memory_monitor:
  enabled: true
  empty_cache: true
  log_frequency: 100
  monitor_cpu: true
  monitor_gpu: true

# Monitoring
monitoring:
  enabled: true
  prometheus_port: 9090
  health_check_path: "/health"

# Tokenizer settings
tokenizer:
  add_special_tokens: true
  path: "tokenizer"
  vocab_size: 32000
