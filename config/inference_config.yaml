generation:
  max_length: 20
  temperature: 1.0   # Lowered temperature for more focused, coherent output
  top_k: 50          # Restricted to the top 50 tokens for sampling
  top_p: 0.9         # Nucleus sampling to cover 90% of the probability mass
  repetition_penalty: 1.2
  do_sample: true

inference:
  model_path: model/best_LuminaLM_model.pt
  seed: 42

logging:
  gradient_logging_frequency: 2000
  log_dir: SPARSA-LM-Base 0.1/logs
  log_frequency: 500
  log_gradients: false
  log_model_weights: false
  log_train_steps: true
  use_wandb: false
  wandb_entity: null
  wandb_project: LuminaLM_Base_Small

memory_monitor:
  empty_cache: true
  enabled: true
  log_frequency: 10
  monitor_cpu: true
  monitor_gpu: true

model:
  hidden_dim: 128                   # Reduced for quick training
  num_layers: 2                      # Fewer layers for speed
  num_heads: 4                      # Reduced to balance
  ff_dim: 256                        # 2x hidden_dim
  dropout: 0.1
  vocab_size: 8000
  max_seq_len: 64
  use_checkpointing: true
  activation: "gelu"
  tie_embeddings: true
  window_size: 4
  global_tokens: 1
  use_reentrant: false

tokenizer:
  add_special_tokens: true
  path: C:/Users/ASUS/Desktop/SPARSA-LM-Base 0.1/data/processed/tokenizer

training:
  amp_scaler_init_scale: 8192
  batch_size: 2
  checkpoint_dir: C:/Users/ASUS/Desktop/SPARSA-LM-Base 0.1/model
  checkpoint_save_frequency: 1
  device: cuda
  early_stopping_patience: 3
  enable_gradient_accumulation: true
  epochs: 10
  eval_every_n_epochs: 1
  gradient_accumulation_steps: 16
  learning_rate: 2e-4
  log_every_n_steps: 100
  lr_scheduler_kwargs:
    min_lr: 1e-6
  max_grad_norm: 1.0
  resume_from_checkpoint: null
  scheduler_type: cosine_with_min_lr
  seed: 42
  use_gradient_checkpointing: false
  use_mixed_precision: true
  warmup_ratio: 0.1
  weight_decay: 0.01
