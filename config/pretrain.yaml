# SPARSA-LM Pretraining Configuration
# AutoRegressive Language Model Pretraining

# Model Configuration
model:
  size: base  # small, base, large, xl
  vocab_size: 32000
  hidden_size: 1024
  intermediate_size: 4096
  num_hidden_layers: 28
  num_attention_heads: 16
  num_key_value_heads: 8
  max_position_embeddings: 2048
  sliding_window: 512
  rms_norm_eps: 1.0e-6
  use_flash_attention: true
  use_gradient_checkpointing: true
  tie_word_embeddings: true

# Training Configuration
training:
  num_epochs: 3
  max_steps: -1  # -1 to use epochs
  per_device_batch_size: 4
  gradient_accumulation_steps: 8
  effective_batch_size: 256

  # Optimizer
  learning_rate: 3.0e-4
  weight_decay: 0.1
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0

  # Learning Rate Schedule
  lr_scheduler: cosine
  warmup_steps: 2000
  min_lr_ratio: 0.1

  # Mixed Precision
  bf16: true
  fp16: false

  # Gradient Checkpointing
  gradient_checkpointing: true

# DeepSpeed Configuration
deepspeed:
  enabled: true
  stage: 2  # ZeRO stage (0, 1, 2, 3)
  offload_optimizer: false
  offload_param: false

# Data Configuration
data:
  tokenizer_path: tokenizer
  max_seq_length: 2048
  streaming: true
  shuffle_buffer_size: 10000
  num_workers: 4
  prefetch_factor: 2

  # Pretrain datasets
  datasets:
    - c4
    - wikipedia
    - openwebtext
    - pile

  # Domain weights
  domain_weights:
    language: 0.35
    code: 0.25
    math: 0.15
    reasoning: 0.15
    other: 0.10

# Checkpointing
checkpointing:
  output_dir: outputs/pretrain
  checkpoint_dir: outputs/pretrain/checkpoints
  save_steps: 1000
  save_total_limit: 5
  resume_from_checkpoint: null

# Logging
logging:
  logging_dir: outputs/pretrain/logs
  logging_steps: 10
  log_level: info
  report_to:
    - wandb
  wandb_project: sparsa-lm-pretrain
  wandb_entity: null
  wandb_run_name: null

# Evaluation
evaluation:
  eval_steps: 500
  eval_accumulation_steps: 1

# Reproducibility
seed: 42
