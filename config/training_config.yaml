# Training Configuration
training:
  device: "cuda"  # Use "cuda" for GPU or "cpu" for CPU
  epochs: 5  # Number of training epochs
  batch_size: 16  # Batch size for training
  learning_rate: 3e-4  # Initial learning rate
  weight_decay: 0.01  # Weight decay for AdamW optimizer
  max_grad_norm: 1.0  # Gradient clipping threshold
  seed: 42  # Random seed for reproducibility
  resume_from_checkpoint: null  # Path to a checkpoint to resume training (set null if starting fresh)
  checkpoint_dir: "checkpoints"  # Directory to save model checkpoints
  scheduler_type: "cosine"  # Scheduler type: "linear", "cosine", "cosine_with_restarts"
  warmup_ratio: 0.1  # Ratio of warmup steps for learning rate scheduler
  gradient_accumulation_steps: 2  # Number of steps for gradient accumulation
  log_every_n_steps: 10  # Log metrics every N steps
  eval_every_n_epochs: 1  # Evaluate the model every N epochs
  early_stopping_patience: 3  # Number of epochs to wait for validation loss improvement before early stopping
  use_mixed_precision: true  # Enable mixed-precision training for improved GPU performance

# Dataset Configuration
dataset:
  train_dataset: "local_train"  # Reference to the dataset name in datasets.yaml
  val_dataset: "local_test"  # Reference to the dataset name in datasets.yaml
  max_seq_len: 512  # Maximum sequence length for tokenization
  num_workers: 4  # Number of workers for data loading
  shuffle: true  # Shuffle the training data
  preprocessing:  # Preprocessing configuration for datasets
    lowercase: true  # Convert all text to lowercase
    min_length: 5  # Minimum number of tokens required for a text sample to be valid

# Tokenizer Configuration
tokenizer:
  path: "C:/Users/ASUS/Desktop/SPARSA-LM-Base 0.1/data/processed/tokenizer"  # Path to tokenizer

# Model Configuration
model:
  num_layers: 8  # Number of transformer layers
  num_heads: 4  # Number of attention heads
  hidden_dim: 256  # Hidden dimension size
  ff_dim: 1024  # Feed-forward layer dimension size
  dropout: 0.1  # Dropout rate
  vocab_size: 32000  # Vocabulary size
  max_seq_len: 512  # Maximum sequence length
  use_checkpointing: false  # Enable gradient checkpointing for memory efficiency
  activation: "gelu"  # Activation function: "relu", "gelu", "silu"
  tie_embeddings: false  # Share embeddings between input and output tokens

# Logging Configuration
logging:
  log_dir: "logs"  # Directory to save logs
  use_wandb: true  # Enable logging to Weights & Biases
  wandb_project: "LuminaLM_Base_0.1"  # Name of the Weights & Biases project
  wandb_entity: null  # Set this to your W&B entity name if applicable
  log_gradients: true  # Log gradient norms to W&B

# Memory Monitoring
memory_monitor:
  enabled: true  # Enable memory usage logging
  log_frequency: 10  # Log memory usage every N steps
