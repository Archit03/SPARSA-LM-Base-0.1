training:
  device: "cuda"         # Use "cuda" for GPU or "cpu" for CPU
  epochs: 5              # Number of training epochs
  batch_size: 4          # Minibatch size
  learning_rate: 0.000005 # Initial learning rate
  weight_decay: 0.01     # Weight decay for AdamW optimizer
  max_grad_norm: 1.0     # Gradient clipping threshold
  seed: 42               # Random seed for reproducibility
  resume_from_checkpoint: null
  checkpoint_dir: "checkpoints"
  checkpoint_save_frequency: 1
  scheduler_type: "linear"        # "linear" scheduling
  warmup_ratio: 0.1               # Ratio of warmup steps for LR scheduler
  gradient_accumulation_steps: 8  # Reduced from 16 for stability
  log_every_n_steps: 10
  eval_every_n_epochs: 1
  early_stopping_patience: 3
  use_mixed_precision: false
  amp_scaler_init_scale: 32768    # Reduced from 65536 for fewer overflows

dataset:
  train_dataset: "local_data"
  max_seq_len: 256
  num_workers: 1
  shuffle: true
  preprocessing:
    lowercase: true
    min_length: 5
  split:
    test_size: 0.2
    random_state: 42

tokenizer:
  path: "C:/Users/ASUS/Desktop/SPARSA-LM-Base 0.1/data/processed/tokenizer"
  add_special_tokens: true

model:
  # 2 layers encoder, 2 layers decoder
  num_layers: 2
  num_heads: 2
  hidden_dim: 128
  ff_dim: 512
  dropout: 0.1
  vocab_size: 4000
  max_seq_len: 256
  use_checkpointing: true
  checkpointing_params:
    use_reentrant: false
  activation: "gelu"
  tie_embeddings: true

logging:
  log_dir: "SPARSA-LM-Base 0.1/logs"
  use_wandb: true
  wandb_project: "LuminaLM_Base_0.1"
  wandb_entity: null
  log_gradients: true
  log_model_weights: true
  log_train_steps: true
  log_frequency: 100
  gradient_logging_frequency: 1000
  
memory_monitor:
  enabled: true
  log_frequency: 10
  monitor_gpu: true
  monitor_cpu: true
