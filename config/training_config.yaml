training:
  device: "cuda"
  epochs: 10                      
  batch_size: 64                      # Further reduced from 16 for stability
  learning_rate: 0.00003             # Reduced to prevent gradient explosion
  weight_decay: 0.01                 
  max_grad_norm: 0.1                 # Stricter gradient clipping
  gradient_accumulation_steps: 32    # Increased to maintain effective batch size
  seed: 42
  resume_from_checkpoint: null
  checkpoint_dir: "model"
  checkpoint_save_frequency: 2
  scheduler_type: "linear_warmup"    # Changed to linear for more stability
  lr_scheduler_kwargs:
    warmup_steps: 2000               # Reduced warmup steps
    min_lr: 1e-7                     # Lower minimum learning rate
  # Removed warmup_ratio to avoid conflict with warmup_steps
  log_every_n_steps: 10
  eval_every_n_epochs: 1
  early_stopping_patience: 5         # Reduced to catch issues earlier
  use_mixed_precision: true
  amp_scaler_init_scale: 8           # Significantly lowered for stability
  # Removed redundant enable_gradient_accumulation
  max_grad_value: 0.1                # Stricter gradient value limiting
  use_noise_injection: true         # Disabled during initial training
  # Noise settings preserved but disabled for now
  noise_type: "mask"
  noise_prob: 0.01                   # Reduced for future use

dataset:
  train_dataset: "local_data"
  max_seq_len: 64
  num_workers: 8
  shuffle: true
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true
  preprocessing:
    lowercase: true
    min_length: 5
  split:
    test_size: 0.1
    random_state: 42

tokenizer:
  path: "C:/Users/ASUS/Desktop/SPARSA-LM-Base 0.1/data/processed/tokenizer"
  add_special_tokens: true

model:
  hidden_dim: 128
  num_layers: 2
  num_heads: 4
  ff_dim: 256
  dropout: 0.2                       # Increased dropout for regularization
  vocab_size: 8000
  max_seq_len: 64
  use_checkpointing: true
  activation: "gelu"
  tie_embeddings: true
  window_size: 16
  global_tokens: 4
  use_reentrant: false
  initializer_range: 0.02            # Added smaller weight initialization
  use_layer_norm: true               # Explicit layer normalization
  normalize_before: true             # Pre-norm architecture

logging:
  log_dir: "SPARSA-LM-Base 0.1/logs_test"
  use_wandb: false
  log_gradients: true
  log_model_weights: false
  log_train_steps: true
  log_frequency: 100                 # Less frequent logging to reduce overhead

memory_monitor:
  enabled: true
  log_frequency: 10
  monitor_gpu: true
  monitor_cpu: true
  empty_cache: true
  max_memory_usage_percentage: 90    # Added to prevent OOM errors

optimizer:                           # Added optimizer section
  type: "adamw"
  eps: 1e-8
  betas: [0.9, 0.999]
  fused: false                       # Set to true if using newer CUDA versions
  gradient_centralization: true      # Helps with gradient stability

extras:
  use_gradient_checkpointing: true   # Memory efficient backprop
  activation_checkpointing: true     # Additional memory savings
  fp16_opt_level: "O1"               # Conservative mixed precision
  deterministic_algorithms: true     # For reproducibility
  detect_anomaly: true               # Enable anomaly detection during initial runs
  empty_cuda_cache_freq: 10          # Clear CUDA cache periodically