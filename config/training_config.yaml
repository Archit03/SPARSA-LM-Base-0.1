# config/training_config.yaml
# SPARSA-LM 360M Production Model - Modern LLaMA-Style Architecture

training:
  device: "cuda"
  epochs: 100
  batch_size: 4
  learning_rate: 3e-4
  weight_decay: 0.1
  max_grad_norm: 1.0
  gradient_accumulation_steps: 8
  seed: 42
  resume_from_checkpoint: null
  checkpoint_dir: "checkpoints/sparsa-360m"
  checkpoint_save_frequency: 1000
  scheduler_type: "cosine_with_min_lr"
  lr_scheduler_kwargs:
    min_lr: 1e-5
  log_every_n_steps: 10
  eval_every_n_epochs: 1
  early_stopping_patience: 5
  use_mixed_precision: true
  bf16: true  # Use bfloat16 instead of fp16
  amp_scaler_init_scale: 65536
  max_grad_value: 10.0
  warmup_ratio: 0.03
  num_workers: 4

  # Multi-GPU settings
  distributed: true
  use_deepspeed: true
  deepspeed_config: "config/deepspeed_config.json"

dataset:
  train_dataset: "huggingface_mix"
  max_seq_len: 2048
  split:
    test_size: 0.05
    random_state: 42

tokenizer:
  type: "sentencepiece"  # Modern SentencePiece tokenizer
  path: "tokenizer"
  vocab_size: 32000  # LLaMA-style vocab size

model:
  # SPARSA-LM 360M - Modern LLaMA-Style Architecture
  # ~360M params: vocab=32000, d_model=1024, layers=28, ff_dim=4096
  architecture: "llama"
  hidden_dim: 1024
  num_layers: 28
  num_heads: 16
  num_kv_heads: 8  # Grouped Query Attention (GQA) for efficiency
  ff_dim: 4096
  ff_multiplier: 4
  dropout: 0.0  # Modern LLMs use no dropout
  vocab_size: 32000
  max_seq_len: 2048

  # Modern architecture features
  use_rms_norm: true  # RMSNorm instead of LayerNorm
  rms_norm_eps: 1e-6
  use_swiglu: true  # SwiGLU activation
  use_rope: true  # Rotary Position Embeddings
  rope_theta: 10000.0
  rope_scaling: null  # For extended context: {"type": "linear", "factor": 2.0}

  # Attention settings
  use_flash_attention: true
  use_sliding_window: true
  sliding_window_size: 512
  attention_bias: false  # No bias in attention projections

  # Efficiency
  use_checkpointing: true
  tie_embeddings: false  # Don't tie for modern models
  initializer_range: 0.02

logging:
  log_dir: "logs/sparsa-360m"
  use_wandb: true
  wandb_project: "SPARSA-LM-360M"
  wandb_entity: null
  log_gradients: false
  log_model_weights: false
  log_train_steps: true
  log_frequency: 10

memory_monitor:
  enabled: true
  log_frequency: 100
  monitor_gpu: true
  monitor_cpu: true
  empty_cache: true
  max_memory_usage_percentage: 95

optimizer:
  type: "adamw"
  eps: 1e-8
  betas: [0.9, 0.95]
  fused: true

extras:
  use_gradient_checkpointing: true
  compile_model: true
  torch_compile_mode: "reduce-overhead"
