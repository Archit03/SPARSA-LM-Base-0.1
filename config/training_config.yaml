training:
  device: "cuda"
  epochs: 5              
  batch_size: 2  # ðŸ”¹ Small batch size for 4GB VRAM stability
  learning_rate: 2e-4  # ðŸ”¹ Kept higher for faster convergence
  weight_decay: 0.01     
  max_grad_norm: 1.0     
  gradient_accumulation_steps: 16  # ðŸ”¹ Ensures training stability
  seed: 42               
  resume_from_checkpoint: null
  checkpoint_dir: "checkpoints"
  checkpoint_save_frequency: 1
  scheduler_type: "cosine_with_min_lr"  # ðŸ”¹ Stable LR decay
  lr_scheduler_kwargs:
    min_lr: 1e-6  # âœ… Keep stable min_lr
  warmup_ratio: 0.1               
  log_every_n_steps: 100  # ðŸ”¹ Reduce logging frequency
  eval_every_n_epochs: 1
  early_stopping_patience: 3
  use_mixed_precision: true        
  amp_scaler_init_scale: 8192  # ðŸ”¹ Reduced to prevent NaN loss issues
  use_gradient_checkpointing: false  # ðŸ”¹ Disabled for small models
  enable_gradient_accumulation: true  
  max_grad_value: 100.0  
  use_noise_injection: true  

dataset:
  train_dataset: "local_data"
  max_seq_len: 64  # ðŸ”¹ Small sequence length for small model
  num_workers: 0  # ðŸ”¹ Reduce to avoid DataLoader stalls
  shuffle: true
  pin_memory: true  # ðŸ”¹ Speed up GPU transfers
  prefetch_factor: 2  # ðŸ”¹ Improve data pipeline efficiency
  persistent_workers: false  # ðŸ”¹ Prevent stuck processes in DataLoader
  preprocessing:
    lowercase: true
    min_length: 5
  split:
    test_size: 0.2
    random_state: 42

tokenizer:
  path: "C:/Users/ASUS/Desktop/SPARSA-LM-Base 0.1/data/processed/tokenizer"
  add_special_tokens: true

model:
  d_model: 48  # ðŸ”¹ Reduced to stay under 100K parameters
  num_layers: 2  # ðŸ”¹ Small Transformer model
  num_heads: 1  # ðŸ”¹ Minimized for efficiency
  ff_dim: 96  # ðŸ”¹ Balanced to stay under 100K
  dropout: 0.1  
  vocab_size: 512  # âœ… Small vocabulary for 5MB dataset
  max_seq_len: 64  # âœ… Adjusted to match dataset
  use_checkpointing: false  # ðŸ”¹ Not needed for small model
  activation: "gelu"  
  tie_embeddings: true  # âœ… Saves parameters by sharing weights
  window_size: 2  
  global_tokens: 0  
  use_reentrant: false  

logging:
  log_dir: "SPARSA-LM-Base 0.1/logs"
  use_wandb: false
  wandb_project: "LuminaLM_Base_Small"
  wandb_entity: null
  log_gradients: false  # ðŸ”¹ Disable gradient logging to speed up training
  log_model_weights: false  # ðŸ”¹ Reduce logging overhead
  log_train_steps: true
  log_frequency: 500  # ðŸ”¹ Reduce frequent logging to avoid slowdowns
  gradient_logging_frequency: 2000

memory_monitor:
  enabled: true
  log_frequency: 10
  monitor_gpu: true
  monitor_cpu: true
  empty_cache: true  # ðŸ”¹ Enable explicit CUDA memory clearance after each batch
