training:
  device: "cuda"  # Use "cuda" for GPU or "cpu" for CPU
  epochs: 5  # Number of training epochs
  batch_size: 8  # Reduced from 4 to 1
  learning_rate: 0.00003  # Initial learning rate
  weight_decay: 0.01  # Weight decay for AdamW optimizer
  max_grad_norm: 1.0  # Gradient clipping threshold
  seed: 42  # Random seed for reproducibility
  resume_from_checkpoint: null  # Path to a checkpoint to resume training (set null if starting fresh)
  checkpoint_dir: "checkpoints"  # Directory to save model checkpoints
  checkpoint_save_frequency: 1  # Save checkpoints every N epochs
  scheduler_type: "cosine_with_restarts"  # Scheduler type: "linear", "cosine", "cosine_with_restarts"
  scheduler_params:  # Advanced scheduler parameters
    T_0: 10
    T_mult: 2
    eta_min: 1e-6
  warmup_ratio: 0.1  # Ratio of warmup steps for learning rate scheduler
  gradient_accumulation_steps: 16  # Increased from 4 to 16 to compensate for smaller batch size
  log_every_n_steps: 10  # Log metrics every N steps
  eval_every_n_epochs: 1  # Evaluate the model every N epochs
  early_stopping_patience: 3  # Number of epochs to wait for validation loss improvement before early stopping
  use_mixed_precision: true  # Enable mixed-precision training for improved GPU performance
  amp_scaler_init_scale: 65536  # Initial scale for GradScaler (useful for fine control)

dataset:
  train_dataset: "local_data"  # Reference to the dataset name in datasets.yaml
  max_seq_len: 256  # Reduced from 512 to match model config
  num_workers: 1  # Number of workers for data loading
  shuffle: true  # Shuffle the training data
  preprocessing:
    lowercase: true  # Convert all text to lowercase
    min_length: 5  # Minimum number of tokens required for a text sample to be valid
  split:
    test_size: 0.2  # Proportion of validation data
    random_state: 42  # Seed for reproducibility

tokenizer:
  path: "C:/Users/ASUS/Desktop/SPARSA-LM-Base 0.1/data/processed/tokenizer"  # Path to tokenizer or Hugging Face model ID
  add_special_tokens: true  # Ensure special tokens are added if missing

model:
  num_layers: 6  # Number of transformer layers
  num_heads: 8  # Number of attention heads
  hidden_dim: 256  # Reduced from 128
  ff_dim: 1024  # Reduced from 512
  dropout: 0.1  # Dropout rate
  vocab_size: 6000  # Vocabulary size
  max_seq_len: 256  # Reduced from 512 to match model config
  use_checkpointing: true  # Enable gradient checkpointing to save memory
  checkpointing_params:  # Add explicit checkpointing parameters
    use_reentrant: false  # Recommended setting
  activation: "gelu"  # Activation function: "relu", "gelu", "silu"
  tie_embeddings: true  # Share embeddings between input and output tokens to reduce parameters

logging:
  log_dir: "SPARSA-LM-Base 0.1/logs"  # Directory to save logs
  use_wandb: true  # Enable logging to Weights & Biases
  wandb_project: "LuminaLM_Base_0.1"  # Name of the Weights & Biases project
  wandb_entity: null  # Set this to your W&B entity name if applicable
  log_gradients: true  # Log gradient norms to W&B
  log_model_weights: true  # Log model weights
  log_train_steps: true  # Log training steps
  log_frequency: 100  # Log every N steps
  gradient_logging_frequency: 1000  # Log gradients every N steps
  
memory_monitor:
  enabled: true  # Enable memory usage logging
  log_frequency: 10  # Log memory usage every N steps
  monitor_gpu: true  # Include GPU memory stats
  monitor_cpu: true  # Include CPU memory stats
